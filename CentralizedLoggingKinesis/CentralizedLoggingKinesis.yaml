# this line lets us use `Fn::ToJsonString` below
Transform: AWS::LanguageExtensions

AWSTemplateFormatVersion: 2010-09-09

Description: CloudFormation Stack For Receiving CloudWatch Log Group [Log Streams] and Streaming Using Kinesis Firehose
Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: Bucket configuration
        Parameters:
          - CreateNewBucket
          - BucketAcl

Parameters:
  CreateNewBucket:
    Type: String
    Default: true
    AllowedValues:
      - true
      - false
  BucketAcl:
    Description: Provide a ACL for S3 bucket.
    Type: String
    Default: Private
    AllowedValues:
      - Private

  OrgId:
    Type: String
    Default: r-zgfd

  CWLogsDestinationName:
    Type: String
    Default: CentralisedLog

  KinesisLogGroupName:
    Type: String
    Default: /aws/kinesis

  KinesisLogStreamName:
    Type: String
    Default: S3Delivery

  KinesisS3BucketPrefix:
    Type: String
    Default: firehose/

  # s3bucketname:
  #   Type: String
  #   Default: liveline-rahul

Conditions:
  CreateNewBucket: !Equals [!Ref CreateNewBucket, "true"]  

Resources:
  S3Bucket:
    Type: AWS::S3::Bucket
    Condition: CreateNewBucket
    DeletionPolicy: Retain
    Properties:
      BucketName: !Join 
        - "-"
        - - ssm
          - larry
          - logging
          - !Sub ${AWS::AccountId}
      Tags:
        - Key: Name
          Value: !Join 
            - "-"
            - - ssm
              - larry
              - logging
              - !Sub ${AWS::AccountId}    
      AccessControl: !Ref BucketAcl
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256    

  KinesisFirehoseStream:
    DependsOn:
      - KinesisFirehoseStreamPolicy
      - S3Bucket
    Type: AWS::KinesisFirehose::DeliveryStream
    Properties:
      ExtendedS3DestinationConfiguration:
        BucketARN: !GetAtt S3Bucket.Arn
        BufferingHints:
          IntervalInSeconds: 60
          SizeInMBs: 50 
        CompressionFormat: UNCOMPRESSED
        Prefix: !Ref KinesisS3BucketPrefix
        # Prefix: Liveline/!{timestamp:YYYY}/rahul/
        ErrorOutputPrefix: Error/
        RoleARN: !GetAtt KinesisFirehoseStreamRole.Arn
        ProcessingConfiguration:
          Enabled: true
          Processors:
            - Parameters:
              - ParameterName: LambdaArn
                ParameterValue: !GetAtt KinesisTransformLambda.Arn 
              Type: Lambda
        CloudWatchLoggingOptions:
          Enabled: true
          LogGroupName: !Ref KinesisLogGroupName
          LogStreamName: !Ref KinesisLogStreamName

  KinesisFirehoseStreamRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Sid: ''
            Effect: Allow
            Principal:
              Service: firehose.amazonaws.com
            Action: 'sts:AssumeRole'

  KinesisFirehoseStreamPolicy:
    DependsOn:
      - S3Bucket
    Type: AWS::IAM::Policy
    Properties:
      PolicyName: kinesis_firehose_stream_policy
      PolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Action:
              - 's3:AbortMultipartUpload'
              - 's3:GetBucketLocation'
              - 's3:GetObject'
              - 's3:ListBucket'
              - 's3:ListBucketMultipartUploads'
              - 's3:PutObject'
            Resource:
              - !Sub 'arn:aws:s3:::${S3Bucket}/*'
              - !Sub 'arn:aws:s3:::${S3Bucket}'
          - Effect: Allow
            Action:
              - 'logs:CreateLogGroup'
              - 'logs:PutLogEvents'
              - 'logs:CreateLogStream'
            Resource: "*"
          - Effect: Allow
            Action:
              - 'lambda:InvokeFunction'
              - 'lambda:GetFunctionConfiguration'
            Resource: "*"         
      Roles:
        - !Ref KinesisFirehoseStreamRole

  KinesisLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Sid: ''
            Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      Policies:
        - PolicyName: KinesisLambdaPolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Resource: "*"
                Action: 
                  - logs:CreateLogGroup
                  - logs:PutLogEvents
                  - logs:CreateLogStream

  # https://github.com/amazon-archives/serverless-app-examples/blob/master/python/kinesis-firehose-process-record-streams-as-source-python/lambda_function.py
  # https://www.derpturkey.com/a-simple-aws-cloudformation-example-with-lambda-and-kinesis/
  KinesisTransformLambda:
    Type: 'AWS::Lambda::Function'
    Properties:
      FunctionName: KinesisDataTransformation
      Handler: index.lambda_handler
      Description: Transformation from .gzip to human readable format
      Role: !GetAtt KinesisLambdaRole.Arn
      # 'arn:aws:iam::082494019291:role/lambda_kinesis_role'
      Runtime: python3.11
      Timeout: 120
      Code:
        ZipFile: |
          import base64
          import json
          import gzip
          import boto3

          def transformLogEvent(log_event):
            """Transform each log event.

            The default implementation below just extracts the message and appends a newline to it.

            Args:
            log_event (dict): The original log event. Structure is {"id": str, "timestamp": long, "message": str}

            Returns:
            str: The transformed log event.
            """
            return log_event['message'] + '\n'

          def processRecords(records):
            for r in records:
              data = loadJsonGzipBase64(r['data'])
              recId = r['recordId']
              # CONTROL_MESSAGE are sent by CWL to check if the subscription is reachable.
              # They do not contain actual data.
              if data['messageType'] == 'CONTROL_MESSAGE':
                yield {
                  'result': 'Dropped',
                  'recordId': recId
                }
              elif data['messageType'] == 'DATA_MESSAGE':
                joinedData = ''.join([transformLogEvent(e) for e in data['logEvents']])
                dataBytes = joinedData.encode("utf-8")
                encodedData = base64.b64encode(dataBytes).decode('utf-8')
                yield {
                  'data': encodedData,
                  'result': 'Ok',
                  'recordId': recId
                }
              else:
                yield {
                  'result': 'ProcessingFailed',
                  'recordId': recId
                }
            
          def splitCWLRecord(cwlRecord):
            """
            Splits one CWL record into two, each containing half the log events.
            Serializes and compreses the data before returning. That data can then be
            re-ingested into the stream, and it'll appear as though they came from CWL
            directly.
            """
            logEvents = cwlRecord['logEvents']
            mid = len(logEvents) // 2
            rec1 = {k:v for k, v in cwlRecord.items()}
            rec1['logEvents'] = logEvents[:mid]
            rec2 = {k:v for k, v in cwlRecord.items()}
            rec2['logEvents'] = logEvents[mid:]
            return [gzip.compress(json.dumps(r).encode('utf-8')) for r in [rec1, rec2]]

          def putRecordsToFirehoseStream(streamName, records, client, attemptsMade, maxAttempts):
           failedRecords = []
           codes = []
           errMsg = ''
           # if put_record_batch throws for whatever reason, response['xx'] will error out, adding a check for a valid
           # response will prevent this
           response = None
           try:
             response = client.put_record_batch(DeliveryStreamName=streamName, Records=records)
           except Exception as e:
             failedRecords = records
             errMsg = str(e)

           # if there are no failedRecords (put_record_batch succeeded), iterate over the response to gather results
           if not failedRecords and response and response['FailedPutCount'] > 0:
             for idx, res in enumerate(response['RequestResponses']):
               # (if the result does not have a key 'ErrorCode' OR if it does and is empty) => we do not need to re-ingest
               if not res.get('ErrorCode'):
                 continue

               codes.append(res['ErrorCode'])
               failedRecords.append(records[idx])

             errMsg = 'Individual error codes: ' + ','.join(codes)

           if failedRecords:
             if attemptsMade + 1 < maxAttempts:
               print('Some records failed while calling PutRecordBatch to Firehose stream, retrying. %s' % (errMsg))
               putRecordsToFirehoseStream(streamName, failedRecords, client, attemptsMade + 1, maxAttempts)
             else:
               raise RuntimeError('Could not put records after %s attempts. %s' % (str(maxAttempts), errMsg))

          def putRecordsToKinesisStream(streamName, records, client, attemptsMade, maxAttempts):
            failedRecords = []
            codes = []
            errMsg = ''
            # if put_records throws for whatever reason, response['xx'] will error out, adding a check for a valid
            # response will prevent this
            response = None

            try:
              response = client.put_records(StreamName=streamName, Records=records)
            except Exception as e:
              failedRecords = records
              errMsg = str(e)

            # if there are no failedRecords (put_record_batch succeeded), iterate over the response to gather results
            if not failedRecords and response and response['FailedRecordCount'] > 0:
              for idx, res in enumerate(response['Records']):
                # (if the result does not have a key 'ErrorCode' OR if it does and is empty) => we do not need to re-ingest
                if not res.get('ErrorCode'):
                  continue

                codes.append(res['ErrorCode'])
                failedRecords.append(records[idx])

              errMsg = 'Individual error codes: ' + ','.join(codes)
            
            if failedRecords:
              if attemptsMade + 1 < maxAttempts:
                print('Some records failed while calling PutRecords to Kinesis stream, retrying. %s' % (errMsg))
                putRecordsToKinesisStream(streamName, failedRecords, client, attemptsMade + 1, maxAttempts)
              else:
                raise RuntimeError('Could not put records after %s attempts. %s' % (str(maxAttempts), errMsg))

          def createReingestionRecord(isSas, originalRecord, data=None):
            if data is None:
              data = base64.b64decode(originalRecord['data'])
            r = {'Data': data}
            if isSas:
              r['PartitionKey'] = originalRecord['kinesisRecordMetadata']['partitionKey']
            return r

          def loadJsonGzipBase64(base64Data):
            return json.loads(gzip.decompress(base64.b64decode(base64Data)))

          def lambda_handler(event, context):
            isSas = 'sourceKinesisStreamArn' in event
            streamARN = event['sourceKinesisStreamArn'] if isSas else event['deliveryStreamArn']
            region = streamARN.split(':')[3]
            streamName = streamARN.split('/')[1]
            records = list(processRecords(event['records']))
            projectedSize = 0
            recordListsToReingest = []

            for idx, rec in enumerate(records):
              originalRecord = event['records'][idx]

              if rec['result'] != 'Ok':
                continue

              # If a single record is too large after processing, split the original CWL data into two, each containing half
              # the log events, and re-ingest both of them (note that it is the original data that is re-ingested, not the 
              # processed data). If it's not possible to split because there is only one log event, then mark the record as
              # ProcessingFailed, which sends it to error output.
              if len(rec['data']) > 6000000:
                cwlRecord = loadJsonGzipBase64(originalRecord['data'])
                if len(cwlRecord['logEvents']) > 1:
                  rec['result'] = 'Dropped'
                  recordListsToReingest.append(
                    [createReingestionRecord(isSas, originalRecord, data) for data in splitCWLRecord(cwlRecord)])
                else:
                  rec['result'] = 'ProcessingFailed'
                  print(('Record %s contains only one log event but is still too large after processing (%d bytes), ' +
                    'marking it as %s') % (rec['recordId'], len(rec['data']), rec['result']))
                del rec['data']
              else:
                projectedSize += len(rec['data']) + len(rec['recordId'])
                # 6000000 instead of 6291456 to leave ample headroom for the stuff we didn't account for
                if projectedSize > 6000000:
                  recordListsToReingest.append([createReingestionRecord(isSas, originalRecord)])
                  del rec['data']
                  rec['result'] = 'Dropped'

            # call putRecordBatch/putRecords for each group of up to 500 records to be re-ingested
            if recordListsToReingest:
              recordsReingestedSoFar = 0
              client = boto3.client('kinesis' if isSas else 'firehose', region_name=region)
              maxBatchSize = 500
              flattenedList = [r for sublist in recordListsToReingest for r in sublist]
              for i in range(0, len(flattenedList), maxBatchSize):
                recordBatch = flattenedList[i:i + maxBatchSize]
                # last argument is maxAttempts
                args = [streamName, recordBatch, client, 0, 20]
                if isSas:
                  putRecordsToKinesisStream(*args)
                else:
                  putRecordsToFirehoseStream(*args)
                recordsReingestedSoFar += len(recordBatch)
                print('Reingested %d/%d' % (recordsReingestedSoFar, len(flattenedList)))

            print('%d input records, %d returned as Ok or ProcessingFailed, %d split and re-ingested, %d re-ingested as-is' % (
              len(event['records']),
              len([r for r in records if r['result'] != 'Dropped']),
              len([l for l in recordListsToReingest if len(l) > 1]),
              len([l for l in recordListsToReingest if len(l) == 1])))

            return {'records': records}

  CWLogsDestination:
    Type: AWS::Logs::Destination
    DependsOn: KinesisFirehoseStream
    Properties:
      DestinationName: !Ref CWLogsDestinationName
      RoleArn: !GetAtt CWLogsDestinationRole.Arn
      TargetArn: !GetAtt KinesisFirehoseStream.Arn
      # !Ref FirehoseArn
      DestinationPolicy:
        Fn::ToJsonString:
          Version: 2012-10-17
          Statement:
            - Effect: Allow
              Principal: 
                AWS: "${AWS::AccountId}"
              Action: logs:PutSubscriptionFilter 
              Resource: "*"
                # - !Sub arn:aws:logs:${AWS::Region}:367521952991:destination:CentralisedLogss
              Condition: 
                StringEquals:
                  aws:PrincipalOrgID: !Ref OrgId

  CWLogsDestinationRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: logs.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: CWLogsDestinationPolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Resource: !GetAtt KinesisFirehoseStream.Arn
                # !Ref FirehoseArn
                Action: firehose:PutRecord*
                # the AWS docs say to grant "firehose:*" -- how weird is that?


Outputs:
  CWDestinationArn:
    Value: !GetAtt CWLogsDestination.Arn

  BucketARN:
    Value: !GetAtt S3Bucket.Arn